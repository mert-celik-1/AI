"""
1. Tokenizasyon Nedir ?

Tokenizasyon metni bilgisayarÄ±n anlayabileceÄŸi parÃ§alara ayÄ±rma iÅŸlemidir. Metin verilerini iÅŸlerken atÄ±lan ilk adÄ±mdÄ±r ve
sonraki tÃ¼m NLP iÅŸlemlerinin temelini oluÅŸturur. Bir metin dizesini daha kÃ¼Ã§Ã¼k birimler olan tokenlara bÃ¶lme iÅŸlemidir.

Tokenlar ÅŸunlar olabilir :
- Kelimeler
- Alt kelimeler (subwords)
- Karakterler
- Noktalama iÅŸaretleri

"Hugging Face is amazing!" â†’ ["Hugging", "Face", "is", "amazing", "!"]

2. Tokenizasyon TÃ¼rleri

2.1 Kelime TabanlÄ± Tokenizasyon

En basit yaklaÅŸÄ±m, metni boÅŸluklara gÃ¶re kelimelere ayÄ±rmaktÄ±r.
Ã–rnek :
"""
text = "Hugging Face is amazing!"
simple_tokens = text.split()
print(f"Basit kelime tokenizasyonu: {simple_tokens}")
# Ã‡Ä±ktÄ±: ['Hugging', 'Face', 'is', 'amazing!'] -> Noktalama iÅŸaretlerini uygun ÅŸekilde alamaz

"""
2.2 Kelime + Noktalama Tokenizasyonu
Ã–rnek :
"""
import re

def tokenize_with_punctuation(text):
    # Noktalama iÅŸaretlerinin Ã¶nÃ¼ne ve arkasÄ±na boÅŸluk ekle
    text = re.sub(r'([.,!?;:])', r' \1 ', text)
    # Birden fazla boÅŸluÄŸu tek boÅŸluÄŸa indir
    text = re.sub(r'\s+', ' ', text).strip()
    return text.split()

text = "Hugging Face is amazing!"
tokens = tokenize_with_punctuation(text)
print(f"Noktalama duyarlÄ± tokenizasyon: {tokens}")
# Ã‡Ä±ktÄ±: ['Hugging', 'Face', 'is', 'amazing', '!'] -> BileÅŸik kelimeler,kÄ±saltmalar ve Ã¶zel durumlar iÃ§in yetersiz kalabilir
# Ã–rneÄŸin "don't" kelimesi "don","","t" olarak ayrÄ±lÄ±rsa anlamsal bÃ¼tÃ¼nlÃ¼ÄŸÃ¼ bozulabilir.

"""
2.3 Sub-word Tokenizasyon (Modern YaklaÅŸÄ±m)

Modern NLP'de en yaygÄ±n olarak kullanÄ±lan tÃ¼rdÃ¼r. Kelimeleri anlamlÄ± alt alt parÃ§alara bÃ¶ler.

-BPE (Byte-Pair Encoding)
En sÄ±k gÃ¶rÃ¼len karakter Ã§iftlerini birleÅŸtirerek tokenler oluÅŸturur. BaÅŸlangÄ±Ã§ta metin tek tek karakterlere bÃ¶lÃ¼nÃ¼r. ArdÄ±ndan en Ã§ok
geÃ§en Ã§iftler adÄ±m adÄ±m birleÅŸtirilir. Bu sayede hem yaygÄ±n kelime parÃ§alarÄ± hem de nadir kelimeler etkili ÅŸekilde temsil edilir. Dil modellerinin
hem kelime hem de kelime parÃ§asÄ± seviyesinde esnek Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar. GPT modelleri kullanÄ±r
Ã–rnek:
"""
from transformers import GPT2Tokenizer

# GPT-2 BPE tokenizer kullanÄ±mÄ±
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

text = "Hugging Face is developing transformers tokenization!"
tokens = tokenizer.tokenize(text)
print(f"BPE tokenizasyonu (GPT-2): {tokens}")
# Ã–rnek Ã§Ä±ktÄ±: ['Hug', 'ging', 'Ä Face', 'Ä is', 'Ä developing', 'Ä transform', 'ers', 'Ä token', 'ization', '!'] -> Ä karakteri boÅŸluÄŸu temsil ediyor.


"""
-WordPiece
Bilinmeyen kelimeleri tanÄ±mlamak iÃ§in kelimeleri olasÄ± en alt birimlere bÃ¶ler. BERT modeli kullanÄ±r. KÃ¼Ã§Ã¼k harflerle Ã§alÄ±ÅŸÄ±r.
ParÃ§alanmÄ±ÅŸ kelimenin Ã¶nÃ¼nde ## iÅŸaretleri varsa bir Ã¶nceki kelimenin devamÄ± olduÄŸuna iÅŸaret eder. Bu, decode iÅŸlemi sÄ±rasÄ±nda boÅŸluk eklenmemesi gerektiÄŸini belirtir.
Ã–rnek :
"""

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

text = "Unbelievable reactions were observed during the experiment."
tokens = tokenizer.tokenize(text)
print(f"WordPiece tokenizasyonu (BERT): {tokens}")
# Ã–rnek Ã§Ä±ktÄ±:  ['unbelievable', 'reactions', 'were', 'observed', 'during', 'the', 'experiment', '.']


"""
SentencePiece
Dil baÄŸÄ±msÄ±z tokenizasyon saÄŸlar ve boÅŸluklarÄ± da token olarak ele alÄ±r. XLM, T5 gibi Ã§ok dilli modellerde kullanÄ±lÄ±r.
BoÅŸluklarÄ± _ olarak iÅŸler. Dil baÄŸÄ±msÄ±z Ã§alÄ±ÅŸÄ±r.
"""
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")

text = "Hugging Face is developing transformers!"
tokens = tokenizer.tokenize(text)
print(f"SentencePiece tokenizasyonu (XLNet): {tokens}")
# Ã–rnek  Ã§Ä±ktÄ± : ['â–Hu', 'gging', 'â–Face', 'â–is', 'â–developing', 'â–transform', 'ers', '!']


"""
3. Hugging Face Tokenizer'larÄ± Derinlemesine Ä°nceleme
3.1 Tokenizer YapÄ±sÄ± ve Ã–zellikleri
Hugging Face tokenizer'larÄ± ÅŸu bileÅŸenleri iÃ§erir:

Vocabulary: Token-ID eÅŸleÅŸtirmeleri
Special tokens: [CLS], [SEP], [MASK] gibi Ã¶zel tokenler
Normalization rules: BÃ¼yÃ¼k-kÃ¼Ã§Ã¼k harf dÃ¶nÃ¼ÅŸÃ¼mÃ¼, aksanlÄ± karakterlerin iÅŸlenmesi
Pre-tokenization rules: Ã–n-tokenizasyon kurallarÄ±
Model-specific encoding: Model-spesifik kodlama kurallarÄ±
"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Tokenizer Ã¶zelliklerini keÅŸfetme
print(f"Vocabulary boyutu: {tokenizer.vocab_size}") # 30522
print(f"Ã–zel tokenler: {tokenizer.all_special_tokens}") # ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']
print(f"Padding token: {tokenizer.pad_token}") # PAD
print(f"Bilinmeyen token: {tokenizer.unk_token}") # UNK
print(f"Model max uzunluÄŸu: {tokenizer.model_max_length}") # 512

# SÃ¶zlÃ¼ÄŸÃ¼n bir kÄ±smÄ±nÄ± gÃ¶rÃ¼ntÃ¼leme
vocab_subset = dict(list(tokenizer.vocab.items())[:10])
print(f"Vocabulary'den Ã¶rnek: {vocab_subset}") # {'[unused924]': 929, 'enclosed': 10837, 'credit': 4923, 'owe': 12533, 'newscast': 20306, 'hansen': 13328, 'gillespie': 21067, 'planting': 14685, 'spat': 14690, '##ity': 3012}


"""
3.2 DetaylÄ± Tokenizasyon SÃ¼reci
Ã–rnekler :
"""

from transformers import BertTokenizer

# BERT tokenizer yÃ¼kleme
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Ã–rnek metinler
texts = [
    "Hugging Face is amazing!",
    "I love NLP and deep learning.",
    "Tokenization is a fundamental step in NLP.",
    "Python is great for machine learning.",
    "The quick brown fox jumps over the lazy dog."
]

# Her metin iÃ§in tokenizasyon sÃ¼recini inceleme
for text in texts:
    print(f"\n\n{'=' * 50}")
    print(f"ORÄ°JÄ°NAL METÄ°N: \"{text}\"")
    print(f"{'=' * 50}")

    # 1. Metni kÃ¼Ã§Ã¼k harfe Ã§evirme (BERT uncased iÃ§in)
    normalized_text = text.lower()
    print(f"\n1. NORMALIZATION:")
    print(f"   {normalized_text}")

    # 2. Basit tokenizasyon (kelime dÃ¼zeyinde)
    simple_tokens = normalized_text.split()
    print(f"\n2. BASÄ°T KELÄ°ME TOKENIZASYONU:")
    print(f"   {simple_tokens}")

    # 3. BERT tokenizer ile tokenizasyon
    tokens = tokenizer.tokenize(text)
    print(f"\n3. BERT WORDPIECE TOKENIZASYONU:")
    print(f"   {tokens}")

    # 4. Token --> ID dÃ¶nÃ¼ÅŸÃ¼mÃ¼
    token_ids = tokenizer.convert_tokens_to_ids(tokens)
    print(f"\n4. TOKEN ID'LERÄ°:")
    print(f"   {token_ids}")

    # 5. Tam encode iÅŸlemi
    encoding = tokenizer(text, return_tensors="pt")
    print(f"\n5. TAM ENCODING (special tokenlar dahil):")
    print(f"   input_ids: {encoding['input_ids'][0].tolist()}")
    print(f"   attention_mask: {encoding['attention_mask'][0].tolist()}")

    # 6. Special tokenlarÄ±n yerleÅŸtirilmesi
    special_tokens_added = tokenizer.build_inputs_with_special_tokens(token_ids)
    print(f"\n6. SPECIAL TOKEN'LAR EKLENMIÅ:")
    print(f"   {special_tokens_added}")
    print(f"   [CLS] token ID: {tokenizer.cls_token_id}")
    print(f"   [SEP] token ID: {tokenizer.sep_token_id}")

    # 7. ID --> Token geri dÃ¶nÃ¼ÅŸÃ¼mÃ¼
    decoded_tokens = tokenizer.convert_ids_to_tokens(special_tokens_added)
    print(f"\n7. ID'LERDEN TOKEN'LARA DÃ–NÃœÅÃœM:")
    print(f"   {decoded_tokens}")

    # 8. Tam decode iÅŸlemi
    decoded_text = tokenizer.decode(special_tokens_added)
    print(f"\n8. TAM DECODE:")
    print(f"   \"{decoded_text}\"")

    # 9. Ã–zel token ID'leri olmadan decode
    decoded_text_clean = tokenizer.decode(special_tokens_added, skip_special_tokens=True)
    print(f"\n9. Ã–ZEL TOKEN'LAR OLMADAN DECODE:")
    print(f"   \"{decoded_text_clean}\"")

    # 10. Alt kelime (subword) tokenlarÄ±n analizi
    print(f"\n10. ALT KELÄ°ME TOKENLARIN ANALÄ°ZÄ°:")
    for token in tokens:
        if token.startswith("##"):
            print(f"   '{token}': Bir kelimenin devamÄ± olan alt token")
        else:
            print(f"   '{token}': Kelime baÅŸlangÄ±cÄ± veya tam kelime")



"""
3.3 FarklÄ± Metinlerin Tokenize Edilmesi ve Analizi
Ã–rnekler :
"""

from transformers import AutoTokenizer
import pandas as pd

# FarklÄ± model tokenizer'larÄ±nÄ± yÃ¼kleme
tokenizers = {
    "BERT": AutoTokenizer.from_pretrained("bert-base-uncased"),
    "RoBERTa": AutoTokenizer.from_pretrained("roberta-base"),
    "GPT-2": AutoTokenizer.from_pretrained("gpt2"),
    "T5": AutoTokenizer.from_pretrained("t5-base"),
    "XLNet": AutoTokenizer.from_pretrained("xlnet-base-cased")
}

# Test metinleri
test_texts = [
    "Hugging Face transformers library is amazing!",  # Normal cÃ¼mle
    "COVID-19 has affected the world economy.",  # SayÄ±lar ve kÄ±saltmalar
    "I don't want to walk 500 miles.",  # Apostroflu kelimeler
    "The email address is example@huggingface.co.",  # E-posta
    "This is a looooooong wooooooord.",  # Tekrarlanan harfler
    "https://huggingface.co is the website URL.",  # URL
    "Python3.8 and #hashtags are special tokens.",  # Ã–zel karakterler
    "She said, 'Hello world!' with excitement.",  # AlÄ±ntÄ±lar
    "à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤¦à¥à¤¨à¤¿à¤¯à¤¾! (Hello world in Hindi)",  # FarklÄ± dil/alfabe
    "This costs $42.50 for 3 items."  # Para birimi ve sayÄ±lar
]

# Her tokenizer ve her metin iÃ§in sonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±rma
results = []

for text_idx, text in enumerate(test_texts):
    for tokenizer_name, tokenizer in tokenizers.items():
        tokens = tokenizer.tokenize(text)
        token_ids = tokenizer.encode(text)

        results.append({
            "Text ID": text_idx + 1,
            "Text": text,
            "Tokenizer": tokenizer_name,
            "Tokens": tokens,
            "Token Count": len(tokens),
            "Has Special Characters": any(c in text for c in "!@#$%^&*()[]{}|\\;:'\",.<>/?"),
            "Has Numbers": any(c.isdigit() for c in text),
            "Avg Token Length": sum(len(t.replace("##", "").replace("â–", "")) for t in tokens) / len(
                tokens) if tokens else 0
        })

# SonuÃ§larÄ± DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼rme
df = pd.DataFrame(results)

# Tokenizer'lar arasÄ±ndaki token sayÄ±sÄ± farklarÄ±nÄ± analiz etme
pivot_counts = df.pivot_table(
    index=["Text ID", "Text"],
    columns="Tokenizer",
    values="Token Count",
    aggfunc="first"
)

print("\nTOKEN SAYILARI KARÅILAÅTIRMASI:")
print(pivot_counts)

# En Ã§ok ve en az tokena bÃ¶lÃ¼nen metinleri bulma
for tokenizer_name in tokenizers.keys():
    max_tokens_row = df[df["Tokenizer"] == tokenizer_name].loc[
        df[df["Tokenizer"] == tokenizer_name]["Token Count"].idxmax()]
    min_tokens_row = df[df["Tokenizer"] == tokenizer_name].loc[
        df[df["Tokenizer"] == tokenizer_name]["Token Count"].idxmin()]

    print(f"\n{tokenizer_name} iÃ§in:")
    print(f"  En Ã§ok token ({max_tokens_row['Token Count']}): \"{max_tokens_row['Text']}\"")
    print(f"  Tokenler: {max_tokens_row['Tokens']}")
    print(f"  En az token ({min_tokens_row['Token Count']}): \"{min_tokens_row['Text']}\"")
    print(f"  Tokenler: {min_tokens_row['Tokens']}")

# Ã–zel durumlar iÃ§in ayrÄ±ntÄ±lÄ± inceleme
print("\n\nÃ–ZEL DURUMLARIN ANALÄ°ZÄ°:")

for text_idx, text in enumerate(test_texts):
    print(f"\n{'-' * 100}")
    print(f"TEXT {text_idx + 1}: \"{text}\"")

    for tokenizer_name, tokenizer in tokenizers.items():
        tokens = tokenizer.tokenize(text)
        print(f"\n{tokenizer_name} tokenizasyonu ({len(tokens)} token):")
        print(f"  {tokens}")

        # Ã–zel analiz
        if "don't" in text and tokenizer_name == "BERT":
            print("  Ã–ZEL ANALÄ°Z: BERT 'don't' kelimesini nasÄ±l tokenize ediyor?")
            dont_indices = [i for i, t in enumerate(tokens) if "don" in t or "'" in t or "t" in t]
            print(f"  Ä°lgili tokenler: {[tokens[i] for i in dont_indices]}")

        if "@" in text:
            print("  Ã–ZEL ANALÄ°Z: Email adresi nasÄ±l tokenize ediliyor?")
            at_index = text.find("@")
            relevant_tokens = [t for t in tokens if any(c in t for c in "@.")]
            print(f"  Ä°lgili tokenler: {relevant_tokens}")

        if "looooooong" in text:
            print("  Ã–ZEL ANALÄ°Z: Tekrarlanan harfler nasÄ±l tokenize ediliyor?")
            long_tokens = [t for t in tokens if "loo" in t or "ooo" in t or "ong" in t]
            print(f"  Ä°lgili tokenler: {long_tokens}")


"""
3.4 Tokenizasyon SÄ±rasÄ±nda YaÅŸanan Zorluklar ve Ã‡Ã¶zÃ¼mleri
Ã–rnek :
"""

from transformers import AutoTokenizer
import pandas as pd

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Tokenizasyon zorluklarÄ±
challenges = [
    {
        "category": "Bilinmeyen kelimeler",
        "example": "Supercalifragilisticexpialidocious is a long word.",
        "challenge": "SÃ¶zlÃ¼kte olmayan kelimeler Ã§ok sayÄ±da alt tokene bÃ¶lÃ¼nÃ¼r."
    },
    {
        "category": "Teknik terimler",
        "example": "PyTorch and TensorFlow are deep learning frameworks.",
        "challenge": "Teknik terimler (PyTorch, TensorFlow) sÄ±k kullanÄ±lmayan alt tokenlere bÃ¶lÃ¼nebilir."
    },
    {
        "category": "YazÄ±m hatalarÄ±",
        "example": "I uesd the wrnog spellnig for words.",
        "challenge": "YazÄ±m hatalarÄ± tokenizasyonu beklenmedik ÅŸekilde etkileyebilir."
    },
    {
        "category": "Emoji ve semboller",
        "example": "I love NLP ğŸ˜ and programming ğŸ’»!",
        "challenge": "Emoji ve Ã¶zel semboller beklenmedik ÅŸekilde tokenize edilebilir."
    },
    {
        "category": "BirleÅŸik kelimeler",
        "example": "HandsOn and hands-on are compound words.",
        "challenge": "FarklÄ± ÅŸekillerde yazÄ±lan birleÅŸik kelimeler farklÄ± tokenize edilebilir."
    },
    {
        "category": "Ã‡ok dilli iÃ§erik",
        "example": "English words and TÃ¼rkÃ§e kelimeler in the same text.",
        "challenge": "FarklÄ± dillerdeki kelimeler karÄ±ÅŸÄ±k bir metinde tokenizasyon sorunlarÄ±na yol aÃ§abilir."
    }
]

# Her zorluk iÃ§in tokenizasyon sonucunu inceleme
results = []

for challenge in challenges:
    tokens = tokenizer.tokenize(challenge["example"])
    token_ids = tokenizer.encode(challenge["example"])

    results.append({
        "Category": challenge["category"],
        "Example": challenge["example"],
        "Challenge": challenge["challenge"],
        "Tokens": tokens,
        "Token Count": len(tokens),
        "Tokens/Words Ratio": len(tokens) / len(challenge["example"].split())
    })

challenge_df = pd.DataFrame(results)
print("\nTOKENIZASYON ZORLUKLARI:")
for idx, row in challenge_df.iterrows():
    print(f"\n{'-' * 80}")
    print(f"KATEGORÄ°: {row['Category']}")
    print(f"Ã–RNEK: \"{row['Example']}\"")
    print(f"ZORLUK: {row['Challenge']}")
    print(f"TOKENLER ({row['Token Count']}): {row['Tokens']}")
    print(f"TOKEN/KELÄ°ME ORANI: {row['Tokens/Words Ratio']:.2f}")

# Ã‡Ã¶zÃ¼mler ve en iyi uygulamalar
print("\n\n" + "=" * 100)
print("TOKENIZASYON SORUNLARI Ä°Ã‡Ä°N Ã‡Ã–ZÃœMLER VE EN Ä°YÄ° UYGULAMALAR")
print("=" * 100)

solutions = [
    {
        "problem": "Alt kelime fragmantasyonu (Bir kelimenin Ã§ok sayÄ±da tokene bÃ¶lÃ¼nmesi)",
        "solution": "Model seÃ§imini gÃ¶rev iÃ§in uygun yapÄ±n. Domain-specific modeller kullanÄ±n. Fine-tuning Ã¶ncesi tokenizer'Ä± domain verilerinizle geniÅŸletin.",
        "example_code": """
# Tokenizer'Ä± domain verileriyle geniÅŸletme Ã¶rneÄŸi
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
domain_vocab = ["supercalifragilisticexpialidocious", "pytorch", "tensorflow"]

# NOT: GerÃ§ek uygulamada, tokenizer sÄ±nÄ±fÄ±nÄ±n Ã¶zelliklerine baÄŸlÄ± olarak 
# bu iÅŸlem farklÄ±lÄ±k gÃ¶sterebilir ve tam olarak desteklenmeyebilir.
"""
    },
    {
        "problem": "Maksimum token uzunluÄŸu sÄ±nÄ±rlarÄ±",
        "solution": "Uzun metinleri pencere yaklaÅŸÄ±mÄ±yla iÅŸleyin, Ã¶rtÃ¼ÅŸen pencereler kullanÄ±n, hiyerarÅŸik modeller deneyin.",
        "example_code": """
# Uzun metinleri pencereli yaklaÅŸÄ±mla iÅŸleme
def process_long_text(text, tokenizer, max_length=512, stride=256):
    tokenized = tokenizer(text, return_tensors="pt")
    input_ids = tokenized["input_ids"][0]

    # Metin Ã§ok uzunsa pencereler oluÅŸtur
    if len(input_ids) > max_length:
        windows = []
        for i in range(0, len(input_ids), stride):
            end_idx = min(i + max_length, len(input_ids))
            windows.append(input_ids[i:end_idx])
            if end_idx == len(input_ids):
                break
        return windows
    else:
        return [input_ids]
"""
    },
    {
        "problem": "Model-spesifik tokenizasyon farklÄ±lÄ±klarÄ±",
        "solution": "Her model iÃ§in doÄŸru tokenizer'Ä± kullanÄ±n. AutoTokenizer kullanÄ±mÄ±nÄ± tercih edin.",
        "example_code": """
# Her zaman model ile uyumlu tokenizer kullanÄ±n
from transformers import AutoTokenizer, AutoModel

model_name = "bert-base-uncased"
# AutoTokenizer otomatik olarak model ile uyumlu tokenizer'Ä± seÃ§er
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
"""
    },
    {
        "problem": "Ã‡oklu dil ve karakter seti sorunlarÄ±",
        "solution": "Ã‡ok dilli modeller kullanÄ±n (XLM-RoBERTa, mBERT) veya SentencePiece tabanlÄ± tokenizer'lar tercih edin.",
        "example_code": """
# Ã‡ok dilli model ve tokenizer kullanÄ±mÄ±
from transformers import AutoTokenizer, AutoModel

# XLM-RoBERTa Ã§ok dilli bir modeldir
model_name = "xlm-roberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# FarklÄ± dillerde metin Ã¶rnekleri
texts = [
    "This is English text.",
    "Dies ist deutscher Text.",
    "ã“ã‚Œã¯æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚",
    "Ğ­Ñ‚Ğ¾ Ñ€ÑƒÑÑĞºĞ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚.",
    "Ù‡Ø°Ø§ Ù†Øµ Ø¹Ø±Ø¨ÙŠ."
]

for text in texts:
    tokens = tokenizer.tokenize(text)
    print(f"Text: {text}")
    print(f"Tokens: {tokens}")
    print()
"""
    }
]

for idx, solution in enumerate(solutions):
    print(f"\n{idx + 1}. PROBLEM: {solution['problem']}")
    print(f"   Ã‡Ã–ZÃœM: {solution['solution']}")
    print(f"   Ã–RNEK KOD:")
    print(f"{solution['example_code']}")

# Tokenizasyon performans deÄŸerlendirmesi
print("\n\n" + "=" * 100)
print("TOKENIZASYON PERFORMANS DEÄERLENDÄ°RMESÄ°")
print("=" * 100)

import time
import numpy as np

# FarklÄ± uzunluklarda metinler oluÅŸturma
text_sizes = [10, 50, 100, 200, 400]
texts = {}

np.random.seed(42)  # Tekrarlanabilirlik iÃ§in
vocab = ["the", "a", "an", "in", "on", "with", "for", "and", "but", "or",
         "if", "because", "although", "while", "when", "where", "what", "who",
         "how", "why", "this", "that", "these", "those", "they", "we", "I", "you",
         "he", "she", "it", "be", "have", "do", "say", "go", "get", "make", "see",
         "know", "take", "come", "think", "look", "want", "give", "use", "find",
         "tell", "ask", "work", "seem", "feel", "try", "leave"]

for size in text_sizes:
    words = np.random.choice(vocab, size=size)
    texts[size] = " ".join(words)

# FarklÄ± tokenizer'larÄ±n performansÄ±nÄ± Ã¶lÃ§me
performance_results = []

for tokenizer_name, tokenizer in tokenizers.items():
    for size, text in texts.items():
        # Tokenizasyon sÃ¼resi Ã¶lÃ§Ã¼mÃ¼
        start_time = time.time()
        tokens = tokenizer.tokenize(text)
        tokenize_time = time.time() - start_time

        # Encoding sÃ¼resi Ã¶lÃ§Ã¼mÃ¼
        start_time = time.time()
        encoding = tokenizer.encode(text, return_tensors="pt")
        encode_time = time.time() - start_time

        performance_results.append({
            "Tokenizer": tokenizer_name,
            "Text Size (words)": size,
            "Token Count": len(tokens),
            "Tokenize Time (ms)": tokenize_time * 1000,
            "Encode Time (ms)": encode_time * 1000,
            "Tokens/Word Ratio": len(tokens) / size
        })

performance_df = pd.DataFrame(performance_results)

print("\nPERFORMANS KARÅILAÅTIRMASI:")
print(performance_df.sort_values(by=["Text Size (words)", "Tokenizer"]))

# Tokenizasyon oranlarÄ± karÅŸÄ±laÅŸtÄ±rmasÄ±
print("\nTOKEN/KELÄ°ME ORANI KARÅILAÅTIRMASI:")
ratio_pivot = performance_df.pivot_table(
    index="Text Size (words)",
    columns="Tokenizer",
    values="Tokens/Word Ratio",
    aggfunc="first"
)
print(ratio_pivot)

# Tokenizasyon sÃ¼releri karÅŸÄ±laÅŸtÄ±rmasÄ±
print("\nTOKENIZASYON SÃœRELERÄ° KARÅILAÅTIRMASI (ms):")
time_pivot = performance_df.pivot_table(
    index="Text Size (words)",
    columns="Tokenizer",
    values="Tokenize Time (ms)",
    aggfunc="first"
)
print(time_pivot)